<html xmlns:v="urn:schemas-microsoft-com:vml" xmlns:o="urn:schemas-microsoft-com:office:office" xmlns="http://www.w3.org/TR/REC-html40">

<head>
<meta http-equiv="Content-Language" content="zh-cn">
<meta name="GENERATOR" content="Microsoft FrontPage 6.0">
<meta name="ProgId" content="FrontPage.Editor.Document">
<meta http-equiv="Content-Type" content="text/html; charset=gb2312">
<title>PERSONAL</title>
<style>
<!--
div.Section1
	{page:Section1;}
SPAN.skype_pnh_container {
	DIRECTION: ltr !important
}
SPAN.skype_pnh_container SPAN.skype_pnh_mark {
	DISPLAY: none !important
}

SPAN.skype_pnh_container IMG.skype_pnh_logo_img {
	MARGIN-RIGHT: 5px !important
}

h3
	{margin-right:0in;
	margin-left:0in;
	font-size:13.5pt;
	font-family:"Times New Roman","serif";
	}
.style1 {font-family: "Times New Roman", Times, serif}
-->
</style>
</head>

<body>

<p style="line-height: 150%" align="justify">
<span lang="EN-US" style="font-family: Times New Roman; font-weight: normal">
<font size="4">
&nbsp;&nbsp;&nbsp; Guoying Zhao is currently a P</font></span><span style="font-family: Times New Roman; font-weight: normal"><font size="4">rofessor 
with the Center for Machine Vision and Signal Analysis, University of Oulu, 
Finland, where she has been a senior researcher since 2005 and an Associate 
Professor since 2014. She received the Ph.D. degree in computer science from the 
Chinese Academy of Sciences, Beijing, China, in 2005.&nbsp; <span lang="en-us">
She got the Academy Postdoctoral position in 2007, i</span>n 2011, she was 
selected to the highly competitive Academy Research Fellow position, and in 
2020, she was selected to one of the ten Academy Professors. She has 
authored or co-authored more than <span lang="en-us">24</span>0 papers in journals and conferences, and 
has served </font></span><font size="4">
<span style="font-family: Times New Roman; font-weight: normal">as a reviewer for many journals and conferences. Her papers have 
currently over 14200 citations in Google Scholar (h-index 
54). She is Co-Program chair of ACM International Conference on Multimodal 
Interaction 2021 (ICMI), Special 
Sessions/Panels Chairs for FG 2023, and general chair for ICBEA 2020. She was general chair for ICBEA 2019, Co-Chair for Late Breaking 
Results of ICMI 2019, co-publicity c</span></font><span style="font-family: Times New Roman; font-weight: normal"><font size="4">hair 
for FG 2018, has served as area chairs for several conferences and is associate 
editor for Pattern Recognition, IEEE Transactions on Circuits and Systems for 
Video Technology, and Image and Vision Computing Journals.</font></span><span lang="EN-US" style="font-size: 12.0pt; line-height: 115%; font-family: Arial,sans-serif">
</span><span style="font-family: Times New Roman; font-weight: normal">
<font size="4">She has lectured tutorials at <span lang="en-us">FG 2018, </span>ICPR 2006, ICCV 2009, 
and SCIA 2013, and authored/edited three books and <span lang="en-us">eight</span> special issues in 
journals. Dr. Zhao was a Co-Chair of 17 International Workshops / special 
sessions in top venues, such as ICCV, 
CVPR, ECCV, ACCV and FG. Her students <span lang="en-us">and 
researchers </span>are 
frequent recipients of very prestigious and highly competitive f</font></span><span style="font-family: Times New Roman; font-weight: normal"><font size="4">ellowships, such 
as <span lang="en-us">Academy of Finland Postdoc position, </span>the Nokia 
Scholarsh</font></span><font size="4"><span style="font-family: Times New Roman; font-weight: normal">ip, 
Endeavour Research Fellowship, Ta</span></font><span style="font-family: Times New Roman; font-weight: normal"><font size="4">uno T&ouml;nning Research funding<span lang="en-us">, 
Kauta Foundation grant</span> 
and Jorma Ollila grant. </font> </span><font size="4">
<span style="font-family: Times New Roman; font-weight: normal">She is IEEE 
Senior Member. Her current research interests include image and video 
descriptors, gait analysis, dynamic-texture recognition, facial-expression 
recognition, human motion analysis, and person identification. Her research has 
been reported by Finnish TV programs, newspapers and MIT Technology Review. </span></font></p>

<table border="0" cellpadding="0" cellspacing="0" style="border-collapse: collapse" bordercolor="#111111" width="100%" id="AutoNumber1">
  <tr>
    <td width="100%">
    <table border="0" width="100%" id="table1" height="325">
		<tr>
			<td width="273" height="39"><font face="Times New Roman"><b>Nationality:</b></font></td>
			<td height="39" width="1385"><font face="Times New Roman"><span lang="en-us">
			P.R.China</span></font></td>
		</tr>
		<tr>
			<td width="273"height="52" valign=top><b><font face="Times New Roman"><span lang="EN-US">
    Address:</span></font></b></td>
			<td height="52" valign=top width="1385">
			<p style="line-height: 150%"><font face="Times New Roman">
			Center for Machine 
    Vision and Signal Analysis,<br>
			P.O.Box 4500 FI-90014 University of Oulu, Finland.</font></td>
		</tr>
		<tr>
			<td width="273"height="37"><font face="Times New Roman"><b>Office:</b></font></td>
			<td height="37" width="1385">
    <p class="MsoNormal"><font face="Times New Roman">TS</font><span lang="en-us"><font face="Times New Roman">302</font></span></p>			</td>
		</tr>
		<tr>
			<td width="273"height="35"><font face="Times New Roman"><b>Phone<span lang="en-us"> 
			Number</span>:</b> </font></td>
			<td height="35" width="1385"><font face="Times New Roman">+358 
			</font><span lang="en-us"><font face="Times New Roman">294487564</font></span></td>
		</tr>
		<tr>
			<td width="273" height="32"><b><font face="Times New Roman"> E_mail:</font></b></td>
			<td height="32" width="1385">
    <p class="MsoNormal"><a href="mailto:firstname.lastname@ee.oulu.fi">
	<span lang="en-us"><font face="Times New Roman">firstname.lastname(at)</font></span><font face="Times New Roman"><span lang="en-us">oulu.fi</span></font></a></p></td>
		</tr>
		<tr>
			<td width="273" height="45"><b>
    		<font face="Times New Roman">Research 
			Interests:</font></b></td>
			<td width="1385" height="45">
    <font face="Times New Roman">Computer Vision, Pattern Recognition, Affective 
	Computing, Digital image &amp; video processing, 
    Human motion analysis, Virtual Reality, Biometrics, etc.</font></td>
		</tr>
		<tr>
			<td width="273"><b><font face="Times New Roman">
			<a target="_blank" href="http://scholar.google.fi/citations?user=hzywrFMAAAAJ">
			Google Scholar</a></font></b></td>
			<td width="1385">
    <span style="font-family: Times New Roman">
	<a target="_blank" href="http://scholar.google.fi/citations?user=hzywrFMAAAAJ">
	http://scholar.google.fi/citations?user=hzywrFMAAAAJ</a></span></td>
		</tr>
		<tr>
			<td width="273" height="5"></td>
			<td height="5" width="1385"></td>
		</tr>
	  </table>
    <p><font face="Times New Roman">
	<img border="0" src="images/bar.bmp" width="561" height="5"></font></p>
	<div style="language:fi;margin-top:5.76pt;margin-bottom:0pt;margin-left:.38in;
text-indent:-.38in;text-align:left;direction:ltr;unicode-bidi:embed;vertical-align:
baseline;mso-line-break-override:restrictions;punctuation-wrap:simple">
		<p style="margin-top: 24px; margin-bottom: 24px">
		<font face="Times New Roman">
		<u><b>Open positions</b></u>: Postdoctoral researchers, PhD students, Master thesis 
		workers.</font></p>
		<p style="margin-top: 24px; margin-bottom: 24px"><u><b>
		<font face="Times New Roman">NEWS:</font></b></u></p>
		<p>
		<font face="Times New Roman">
		2020.10: Mr. Muzammil Behzad won the three minutes PhD thesis 
		competition organized by ICIP2020:</font><span lang="EN-US" style="font-family: Times New Roman; color: #1F497D">
		<a style="color: blue; text-decoration: underline; text-underline: single" href="https://2020.ieeeicip.org/3-minute-thesis-competition/">
		https://2020.ieeeicip.org/3-minute-thesis-competition/</a>.</span><p>
		<font face="Times New Roman">
		2020.08: We have got the 2nd Place on Action Recognition Track of ECCV 
		2020 VIPriors Challenges (with accuracy of 88.31%). More details can be 
		found
		<a target="_blank" href="https://competitions.codalab.org/competitions/23706#results">
		here</a><span lang="en-us">.</span></font><p>
		<font face="Times New Roman">
		2020.08: Dr. Xiaobai Li has been selected to Assistant Professor (Tenure 
		Track) position in CMVS, University of Oulu.</font><p>
		<font face="Times New Roman">
		2020.07: Dr. Jingang Shi has been appointed Associate Professor position 
		in Xi'an Jiaotong University, China.</font></div>
		<p><span lang="EN-US" style="font-family: Times New Roman">2020.06: 
	We have won the <b>first place</b> in the <b>ChaLearn multi-modal face 
	anti-spoofing attack detection challenge</b> @ CVPR 2020, and the <b>second 
	place </b>in the <b>ChaLearn single-modal face anti-spoofing attack 
	detection challenge</b> @CVPR 2020. More details:
	<a style="color: #0563C1; text-decoration: underline; text-underline: single" href="https://sites.google.com/qq.com/face-anti-spoofing/winners-results/challengecvpr2020">
	https://sites.google.com/qq.com/face-anti-spoofing/winners-results/challengecvpr2020</a>.</span></p>
	<p>
		<font face="Times New Roman">
		2020.06: Joint work with Learning &amp; Educational Technology Research Unit 
	(LET) accepted to top tier IEEE Transactions on Affective Computing: &quot;Muhterem 
	Dindar, Sanna J&auml;rvel&auml;, Sara Ahola, Xiaohua Huang, Guoying Zhao. <b>Leaders 
	and followers identified by emotional mimicry during collaborative learning: 
	A facial expression recognition study on emotional valence</b>.&quot;</font></p>
	<p>
		<font face="Times New Roman">
		2020.06: Ms. Yingyue Xu has successfully defended her doctoral 
	dissertation and obtained PhD degree.</font></p>
	<p>
		<font face="Times New Roman">
		2020.05: Dr. Xin Liu has been selected to highly competitive Academy of 
	Finland postdoctoral position 2020.09-2023.08.</font></p>
	<p style="margin-top: 24px; margin-bottom: 24px"><b><u><font face="Times New Roman">
	<span lang="en-us">Downloading: Databases:</span></font></u></b></p>
	<p><span style="font-family: Times New Roman; font-weight:700" lang="en-us">
	Databases:</span></p>
	<ul>
		<li><span style="font-family: Times New Roman" lang="en-us"><b>OuluVS 
		database</b>: </span>
		<span style="font-size: 12.0pt; font-family: 'Times New Roman',serif">It 
		includes the video and audio data for 20 subjects uttering ten 
		phrases:&nbsp;Hello, Excuse me, I am sorry, Thank you, Good bye, See you, 
		Nice to meet&nbsp;you, You are welcome, How are you, Have a good time. Each 
		person&nbsp;spoke each&nbsp;phrase five times.&nbsp;There are also videos with head 
		motion from front to left, from&nbsp;front to right, without utterance, five 
		times for each person. </span>
		<span lang="en-us" style="font-size: 12.0pt; font-family: 'Times New Roman',serif">
		The details and the baseline results for visual speech recognition can 
		be found in:</span></li>
	</ul>
	<p class="MsoNormal"><span style="font-family: Times New Roman">Zhao G, 
	Barnard M &amp; Pietik&auml;inen M (2009). Lipreading with local spatiotemporal 
	descriptors. IEEE Transactions on Multimedia 11(7):1254-1265.</span></p>
	<p class="MsoNormal">
	<span style="font-size: 12.0pt; font-family: 'Times New Roman',serif">The 
	database can be used, for example,&nbsp; in studying the visual speech 
	recognition (lipreading).<span style="font-family: Times New Roman,serif; font-size:12.0pt"><span lang="en-us">
	</span>If yo</span></span><span style="font-family: Times New Roman" lang="en-us">u 
	want to get a copy, please contact
	<a href="mailto:firstname.lastname@oulu.fi">me</a>.</span></p>
	<ul>
		<li>
		<p class="MsoNormal">
		<span style="font-family: Times New Roman" lang="en-us"><b>Oulu-CASIA 
		NIR&amp;VIS facial expression database</b>: </span>
		<span style="font-size: 12.0pt; font-family: 'Times New Roman',serif">It 
		contains videos with the six typical expressions&nbsp; (happiness, 
		sadness, surprise, <br>anger, fear, disgust) from 80 subjects captured 
		with two imaging systems, NIR (Near Infrared) and VIS (Visible light), 
		under three different<span lang="en-us">
		</span>illumination conditions: normal indoor illumination, weak 
		illumination (only computer display is on) and dark illumination (all 
		lights are off).<span lang="en-us"> </span>The database can be used, for 
		example,&nbsp; in studying the effects of illumination variations<span lang="en-us"> </span>
		to facial expressions, cross-imaging-system facial expression 
		recognition or face recognition.</span></p>
		</li>
	</ul>
	<p class="MsoNormal">
	<span style="font-family: Times New Roman" lang="en-us">This database has 
	been released. If you are interested, please contact
	<a href="mailto:firstname.lastname@oulu.fi">me</a>.</span></p>
	<div dir="ltr" id="page" lang="en">
		<div dir="ltr" id="content" lang="en">
			<ul>
				<li>
				<p class="line891"><b>
				<span style="font-family: Times New Roman">
				<a href="http://www.cse.oulu.fi/SPOSDatabase">SPOS database</a>				</span></b>
				<span style="font-size: 12.0pt; font-family: 'Times New Roman',serif">
				- spontaneous and posed facial expressions database </span>
				<span id="line-35" class="anchor"></span></li>
			</ul>
			<span id="bottom" class="anchor"></span></div>
	</div>
	<div dir="ltr" id="page0" lang="en">
		<div dir="ltr" id="content0" lang="en">
			<p class="line862"><span style="font-family: Times New Roman">SPOS 
			database includes spontaneous and posed facial expressions of 7 
			subjects. Emotional movie clips were shown to subjects to induce 
			spontaneous facial expressions, which include six categories of 
			basic emotions (happy, sad, anger, surprise, fear disgust). Subjects 
			were also asked to pose the six kinds of facial expressions after 
			watching movie clips. Data are recorded by both visual and near 
			infer-red camera. All together 84 posed and 147 spontaneous facial 
			expression clips were labeled out from the starting point to the 
			apex. </span></p>
			<p class="line874"><span style="font-family: Times New Roman">So 
			far, spontaneous and posed facial expressions are usually found in 
			different databases. The difference between databases (different 
			experimental setting and different participants) hindered researches 
			which considered both spontaneous and posed facial expressions. This 
			database offers data collected on the same participants and under 
			the same recording condition, which can be used for comparing or 
			distinguishing spontaneous and posed facial expressions</span></div>
	</div></td>
  </tr>
  </table>


</body>

</html>